{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Best_Practice_for_Views.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlisonJD/RTACourse/blob/main/6_Best_Practice_for_Views.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmZc98PsjKVm",
        "outputId": "9d6ba220-cd29-4f19-dbfa-7e288e23c531"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "% cd \"/content/gdrive/My Drive/Colab Notebooks/Tinybird\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/Tinybird\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9pconZIlBud"
      },
      "source": [
        "def write_text_tofile(filename, text):\n",
        "  with open(filename, 'w') as f:\n",
        "      f.write(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv2TrFRve5-g",
        "outputId": "fe3cd9a5-3577-458f-f779-652913dd38a6"
      },
      "source": [
        "!pip install tinybird-cli -q\n",
        "!tb auth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copy the admin token from https://ui.tinybird.co/tokens and paste it here: \n",
            "\u001b[92m** Auth successful! \n",
            "** Configuration written to .tinyb file, consider adding it to .gitignore\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYOIoAgqgWb8"
      },
      "source": [
        "# 6. Best Practices for Views\n",
        "Views are a really powerful tool to analyze large amounts of data really quickly. Normal views do not store any data, they just perform a read from another table on each access. Materialized views store data transformed by the corresponding SELECT query. Live views store the result of the corresponding SELECT query and are updated any time the result of the query changes. \n",
        "\n",
        "We usually talk about materialized and live views. Here we will focus on materialized views here because they really help solving some real-time problems.\n",
        "\n",
        "Materialized views are useful because if you frequently run the same query on a table it makes sense to run the query once, save the result and just fetch that result. They have two downsides:\n",
        " - They need space.\n",
        " - They add management work (especially recalculating them when new data comes in). \n",
        " - They need to be updated and changed if the source schema changes.\n",
        " - They generally only work for some use cases.\n",
        "\n",
        "\n",
        "Let’s see how to manage these issues and leverage views to get 100-1000x improvements.\n",
        "\n",
        "Views are a way to trade memory and disk to save CPU. That means we will have more things in disk/memory but when we need to use them the CPU will need to fetch less data or do less work than if they didn’t exist.\n",
        "\n",
        "This means spending more money on disk and less money on CPUs. This is a very good tradeoff since, in the cloud, each CPU can be around 25x more expensive than each GB of disk.\n",
        "\n",
        "Here we see some use cases and best practices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFUo_73kgvqn"
      },
      "source": [
        "## Denormalization on Ingest\n",
        "This is one of the most common use cases. It means calculating things when the data arrives (on ingestion time) so that you save a version of your original data with extra/enriched/prepared data. You could do the same operations at query time but when you do them on ingest time you execute the operation once and query many times (so the query time is less than it would be without the view).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpX5IY6Ug4gq"
      },
      "source": [
        "### Join with a Dimensions Dataset\n",
        "Here we use the New York City Taxi Trip dataset and the join table `taxi__zone_lookup` to add columns for the pick-up and drop-off boroughs from the dimensions table to the main table using a pipe. The datasources are created [here](https://colab.research.google.com/drive/1dvZAaHs-Xp3QpCUOPpl_Nb97YilLLZp2?usp=sharing). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr5-FliEkqhG"
      },
      "source": [
        "filename = 'pipes/denormalize_taxi.pipe'\n",
        "text='''\n",
        "NODE denormalized\n",
        "SQL >\n",
        "    SELECT \n",
        "    *,\n",
        "    joinGet('taxi__zone_lookup', 'borough', dolocationid) doborough,\n",
        "    joinGet('taxi__zone_lookup', 'borough', pulocationid) puborough\n",
        "    FROM taxi\n",
        "\n",
        "'''\n",
        "\n",
        "write_file(filename, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwqR7IBklNjy",
        "outputId": "1983c301-762c-4960-e16b-98ea2b8da9b6"
      },
      "source": [
        "!cat pipes/denormalize_taxi.pipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "NODE denormalized\n",
            "SQL >\n",
            "    SELECT \n",
            "    *,\n",
            "    joinGet('taxi__zone_lookup', 'borough', dolocationid) doborough,\n",
            "    joinGet('taxi__zone_lookup', 'borough', pulocationid) puborough\n",
            "    FROM taxi\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrueox7bhi6p",
        "outputId": "f97657b0-c7e3-4fd6-9a8f-ac222bf58038"
      },
      "source": [
        "!tb push --populate --force pipes/denormalize_taxi.pipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m** Processing pipes/denormalize_taxi.pipe\u001b[0m\n",
            "\u001b[0m** Building dependencies\u001b[0m\n",
            "\u001b[0m** Running denormalize_taxi \u001b[0m\n",
            "\u001b[92m** => Test endpoint at https://api.tinybird.co/v0/pipes/denormalize_taxi.json\u001b[0m\n",
            "\u001b[92m** 'denormalize_taxi' created\u001b[0m\n",
            "\u001b[0m** Not pushing fixtures\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPzGukrLhDxH",
        "outputId": "4bd7eac6-f075-46b7-be6d-90f5aa7c86db"
      },
      "source": [
        "!tb sql \"SELECT * FROM denormalize_taxi LIMIT 1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "\u001b[1;32mvendorid:\u001b[0m 1\n",
            "\u001b[1;32mtpep_pickup_datetime:\u001b[0m 2019-08-17 12:04:03\n",
            "\u001b[1;32mtpep_dropoff_datetime:\u001b[0m 2019-08-17 12:14:41\n",
            "\u001b[1;32mpassenger_count:\u001b[0m 1\n",
            "\u001b[1;32mtrip_distance:\u001b[0m 1.5\n",
            "\u001b[1;32mratecodeid:\u001b[0m 1\n",
            "\u001b[1;32mstore_and_fwd_flag:\u001b[0m N\n",
            "\u001b[1;32mpulocationid:\u001b[0m 68\n",
            "\u001b[1;32mdolocationid:\u001b[0m 164\n",
            "\u001b[1;32mpayment_type:\u001b[0m 1\n",
            "\u001b[1;32mfare_amount:\u001b[0m 9\n",
            "\u001b[1;32mextra:\u001b[0m 2.5\n",
            "\u001b[1;32mmta_tax:\u001b[0m 0.5\n",
            "\u001b[1;32mtip_amount:\u001b[0m 2.45\n",
            "\u001b[1;32mtolls_amount:\u001b[0m 0\n",
            "\u001b[1;32mimprovement_surcharge:\u001b[0m 0.3\n",
            "\u001b[1;32mtotal_amount:\u001b[0m 14.75\n",
            "\u001b[1;32mcongestion_surcharge:\u001b[0m 2.5\n",
            "\u001b[1;32mdoborough:\u001b[0m Manhattan\n",
            "\u001b[1;32mpuborough:\u001b[0m Manhattan\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyS93lHxzrgo"
      },
      "source": [
        "### Precalculate Something that would Take a Lot of CPU cycles\n",
        "Imagine that you have a column that is calculated from others in the table. You could calculate it on ingestion time to avoid running those calculations during the query. \n",
        "\n",
        "In this simple example the estimated trip fare is calculated although calculations are usually more complex than just a multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bah0K17h0CUg",
        "outputId": "ece07ea4-29cd-48c5-d3d9-442372f4a949"
      },
      "source": [
        "!tb sql --stats \"SELECT avg(fare_amount/trip_distance) AS avg_per_mile FROM taxi WHERE trip_distance > 0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m** Query took 0.074803347 seconds\n",
            "** Rows read: 84,152,418\n",
            "** Bytes read: 673.22 MB\u001b[0m\n",
            "---------------------\n",
            "|      \u001b[1;32mavg_per_mile\u001b[0m |\n",
            "---------------------\n",
            "| 7.807361125726695 |\n",
            "---------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYbsQFj1_ak"
      },
      "source": [
        "If we create a materialized view of the taxi table with an additional column for the  estimated fare then we can run fast queries without the calculation.\n",
        "\n",
        "First we create the taxi_mv datasource with the additional column for `estimated_fare`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKmbNJ93sps"
      },
      "source": [
        "filename=\"datasources/taxi_mv.datasource\"\n",
        "text='''\n",
        "SCHEMA >\n",
        "    `estimated_fare` Float32,\n",
        "    `vendorid` Int16,\n",
        "    `tpep_pickup_datetime` DateTime,\n",
        "    `tpep_dropoff_datetime` DateTime,\n",
        "    `passenger_count` Nullable(Int16),\n",
        "    `trip_distance` Float32,\n",
        "    `ratecodeid` Nullable(Int16),\n",
        "    `store_and_fwd_flag` String,\n",
        "    `pulocationid` Int32,\n",
        "    `dolocationid` Int32,\n",
        "    `payment_type` Nullable(Int16),\n",
        "    `fare_amount` Float32,\n",
        "    `extra` Float32,\n",
        "    `mta_tax` Float32,\n",
        "    `tip_amount` Float32,\n",
        "    `tolls_amount` Float32,\n",
        "    `improvement_surcharge` Float32,\n",
        "    `total_amount` Float32,\n",
        "    `congestion_surcharge` Nullable(Float32)\n",
        "\n",
        "ENGINE \"MergeTree\"\n",
        "ENGINE_PARTITION_KEY \"toMonth(tpep_pickup_datetime)\"\n",
        "ENGINE_SORTING_KEY \"tpep_pickup_datetime\"\n",
        "'''\n",
        "\n",
        "write_file(filename, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiZCpCC4105f",
        "outputId": "210ad2d7-8b91-4e3e-dfb3-03215d239bcc"
      },
      "source": [
        "!tb push datasources/taxi_mv.datasource"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m** Processing datasources/taxi_mv.datasource\u001b[0m\n",
            "\u001b[0m** Building dependencies\u001b[0m\n",
            "\u001b[0m** Running taxi_mv \u001b[0m\n",
            "\u001b[92m** 'taxi_mv' created\u001b[0m\n",
            "\u001b[0m** Not pushing fixtures\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcQ6clzB2Rko"
      },
      "source": [
        "filename = 'pipes/cost_estimate.pipe'\n",
        "text = '''\n",
        "NODE cost_estimate_node\n",
        "SQL >\n",
        "\n",
        "    SELECT\n",
        "trip_distance * 7.807361125727003 AS estimated_fare,\n",
        "        *\n",
        "    FROM taxi\n",
        "\n",
        "TYPE materialized\n",
        "DATASOURCE taxi_mv\n",
        "'''\n",
        "\n",
        "write_file(filename, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5zyfr_33Mlr",
        "outputId": "6175efb6-2d8a-4447-c83e-ca2dfb8ed45b"
      },
      "source": [
        "!tb push pipes/cost_estimate.pipe --skip-table-checks --populate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m** Processing pipes/cost_estimate.pipe\u001b[0m\n",
            "\u001b[0m** Building dependencies\u001b[0m\n",
            "\u001b[0m** Running cost_estimate \u001b[0m\n",
            "\u001b[0m** Materialized node 'cost_estimate_node' used the Data Source 'taxi_mv'\u001b[0m\n",
            "\u001b[0m** Populating job url https://api.tinybird.co/v0/jobs/b8068369-f142-4a47-b21c-57c960343b85\u001b[0m\n",
            "\u001b[92m** 'cost_estimate' created\u001b[0m\n",
            "\u001b[0m** Not pushing fixtures\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgUkjGTG3STn",
        "outputId": "474cd55f-2711-4b7b-c11a-4f9142f8b802"
      },
      "source": [
        "!tb sql --stats \"SELECT estimated_fare, fare_amount FROM cost_estimate LIMIT 15\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m** Query took 0.001537145 seconds\n",
            "** Rows read: 15\n",
            "** Bytes read: 120 bytes\u001b[0m\n",
            "--------------------------------\n",
            "| \u001b[1;32mestimated_fare\u001b[0m | \u001b[1;32mfare_amount\u001b[0m |\n",
            "--------------------------------\n",
            "|      3.3571653 |         3.5 |\n",
            "|      25.139704 |        13.5 |\n",
            "|              0 |        11.5 |\n",
            "|              0 |         2.5 |\n",
            "|              0 |           0 |\n",
            "|              0 |           0 |\n",
            "|              0 |           0 |\n",
            "|              0 |           0 |\n",
            "|       67.84596 |          34 |\n",
            "|      112.89444 |          40 |\n",
            "|      11.320674 |          10 |\n",
            "|       24.04667 |          11 |\n",
            "|      26.232733 |        14.5 |\n",
            "|      14.521692 |          10 |\n",
            "|       8.666171 |           9 |\n",
            "--------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex79qpR96Er7"
      },
      "source": [
        "The estimation algorithm is not very good 😅 but the query is super fast and reads little data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxVaIyFKFWUc"
      },
      "source": [
        "### Precalculate Something that will Help when Joining with other Datasets at Query Time\n",
        "\n",
        "If you have a join like\n",
        "\n",
        "```\n",
        "SELECT * \n",
        "FROM A \n",
        "JOIN B \n",
        "ON A.id = B.id and A.foo = B.foo\n",
        "-- using id, foo\n",
        "```\n",
        "\n",
        "As we saw, you could have an extra column with id + foo so the query would be\n",
        "\n",
        "```\n",
        "SELECT * \n",
        "FROM A \n",
        "JOIN B \n",
        "ON A.foo_id = B.foo_id\n",
        "WHERE attr = 1\n",
        "````\n",
        "\n",
        "This speeds up the join because you just read one parameter and just check once. Not only that, it also enables (if your database supports it) hash join which is much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_m4MmQOE-qQ"
      },
      "source": [
        "### JSON Data Denormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL4egl8qGACl"
      },
      "source": [
        "## Use Arrays to Denormalize\n",
        "A way to save a lot of rows is to use arrays. Mostly for secondary use cases or the ones you would join after a filter.\n",
        "\n",
        "For example, if we have a table `user_events` with all the mouse clicks or taps during a session, it might make sense to save all of them in the same row using an array instead of saving one row per click. In other words, it’s a way to save a join.\n",
        "\n",
        "The downside is that databases don’t usually have inverted indices so you might want to generate another column or columns based on array content to be able to index.\n",
        "\n",
        "In this example, if we define the table as:\n",
        "```\n",
        "UserId UInt64\n",
        "Timestamp DateTime\n",
        "ClickPoints Array(Int32, Int32)\n",
        "```\n",
        "Then we could do a query to search clicks on a particular zone from people older than 70:\n",
        "```\n",
        "SELECT x, y from ( \n",
        "SELECT * FROM user_events \n",
        "JOIN user_info \n",
        "USING UserId\n",
        "WHERE user_info.age > 70\n",
        ") array join ClickPoints as (x, y)\n",
        "WHERE x between 10 and 100\n",
        "```\n",
        "In this case, if we have, for example, 50 clicks per user as an average, the join will have to do 50x less work. Then the array join generates one entry for each element of the ClickPoints array and we can filter by x, y.\n",
        "\n",
        "Take into account that this is useful in two main situations:\n",
        " - When it allows you to save joins or other expensive operations. If you always need to do the array join this way to store data then it doesn’t make any sense.\n",
        " - When you only need the array join for a small % of use cases in which you use the table. Mainly because you optimize the main use case making worse the secondary use cases but improving the timing overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blD6noAWG9Jm"
      },
      "source": [
        "## Incremental Views\n",
        "One of the main problems of views is updating them. That’s why you need to pick a database that allows you to update them incrementally. It does not make any sense to update a view of a 4 billion row table every time you append more data.\n",
        "\n",
        "So how do you update a view that generates an aggregation? For example,\n",
        "```\n",
        "SELECT date, avg(response_time), uniq(user_id) \n",
        "FROM events\n",
        "GROUP BY date\n",
        "````\n",
        "You might think the view needs to be fully updated if you append a new event with a new `user_id`. \n",
        "\n",
        "You don’t, there are ways to do this really simply. Let’s see an example with the Yandex `hits` dataset. It contains tracking information (like Google Analytics)"
      ]
    }
  ]
}